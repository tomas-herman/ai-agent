{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules and classes\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from typing import Dict\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Set environment variable from a .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.0)\n",
    "# llm = OpenAI(temperature=0.0)\n",
    "\n",
    "# Load and split PDF document into pages\n",
    "loader = PyPDFLoader(\"docs/QMproceedings.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Split text into chunks with specified size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "# Create a Chroma vector store from the document splits\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Create a retriever with a specified number of chunks to retrieve\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 3, \"fetch_k\": 6})\n",
    "# compressor = LLMChainExtractor.from_llm(llm)\n",
    "# retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor=compressor,\n",
    "#     base_retriever=vectorstore.as_retriever(search_type=\"mmr\")\n",
    "# )\n",
    "\n",
    "# Define a chat prompt template for question answering\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Use the following pieces of context to answer the user's question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer:\\n\\n{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a document chain for handling the retrieval and answer process\n",
    "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)\n",
    "\n",
    "# Function to parse the last message from retrieval input\n",
    "def parse_retriever_input(params: Dict):\n",
    "    return params[\"messages\"][-1].content\n",
    "\n",
    "# Define the retrieval chain with context parsing and document chain\n",
    "retrieval_chain = RunnablePassthrough.assign(\n",
    "    context=parse_retriever_input | retriever,\n",
    ").assign(\n",
    "    answer=document_chain,\n",
    ")\n",
    "\n",
    "# Define a prompt for transforming search queries\n",
    "query_transform_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a chain for transforming queries and retrieving documents\n",
    "query_transforming_retriever_chain = RunnableBranch(\n",
    "    (\n",
    "        lambda x: len(x.get(\"messages\", [])) == 1,\n",
    "        (lambda x: x[\"messages\"][-1].content) | retriever,\n",
    "    ),\n",
    "    query_transform_prompt | chat | StrOutputParser() | retriever,\n",
    ").with_config(run_name=\"chat_retriever_chain\")\n",
    "\n",
    "# Define the final conversational retrieval chain\n",
    "conversational_retrieval_chain = RunnablePassthrough.assign(\n",
    "    context=query_transforming_retriever_chain,\n",
    ").assign(\n",
    "    answer=document_chain,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a chat history and perform conversation retrieval\n",
    "chat_history = ChatMessageHistory()\n",
    "chat_history.add_user_message(\"What measurements are repoted in the conference proceedings?\")\n",
    "response = conversational_retrieval_chain.invoke(\n",
    "    {\"messages\": chat_history.messages},\n",
    ")\n",
    "print(response[\"answer\"])\n",
    "chat_history.add_ai_message(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation with another query\n",
    "chat_history.add_user_message(\"Which models agree best with the results?\")\n",
    "response = conversational_retrieval_chain.invoke(\n",
    "    {\"messages\": chat_history.messages},\n",
    ")\n",
    "print(response[\"answer\"])\n",
    "chat_history.add_ai_message(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
